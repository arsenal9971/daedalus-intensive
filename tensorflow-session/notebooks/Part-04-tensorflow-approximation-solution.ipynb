{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating functions with deep ReLU networks\n",
    "*Practical session during the [DEADALUS Introductory Intensive Course](https://daedalus-berlin.github.io/events.html), December, 2018.*\n",
    "The content is mostly based on [D. Yarotsky, 2017](https://www.sciencedirect.com/science/article/pii/S0893608017301545).\n",
    "\n",
    "## Part IV: Approximating functions with Tensorflow networks (solution)\n",
    "\n",
    "Let us now realize our approximation theoretic thoughts in Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import target_functions as tfunc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,6)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup\n",
    "\n",
    "Lets start with defining some constants, settings, configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "DOMAIN = [0.0, 1.0, 0.0, 1.0]   # unit square [0, 1] x [0, 1]\n",
    "RESOLUTION = 200                # grid resolution for testing the trained model\n",
    "\n",
    "# network settings\n",
    "WIDTH = 20              # number of neurons per hidden layer\n",
    "DEPTH = 6               # total number of layers (number of hidden layers is DEPTH-2)\n",
    "ACTIVATION = tf.nn.relu # hidden layer activation function\n",
    "\n",
    "# training settings\n",
    "INIT_L_RATE  = 5e-3\n",
    "FINAL_L_RATE = 1e-4\n",
    "NUM_ITER     = 2000\n",
    "BATCH_SIZE   = 2048\n",
    "\n",
    "# setup target function\n",
    "REGULARITY = [2, 3]\n",
    "def target_func(x, y):\n",
    "    smooth_part = 2*tfunc.bernstein2d(x, y, [2, 3], [3, 4])\n",
    "    nonsmooth_part = 2*np.math.factorial(REGULARITY[0]) \\\n",
    "        * np.math.factorial(REGULARITY[1]) \\\n",
    "        * tfunc.signpoly2d(x-0.4, y-0.6, REGULARITY)\n",
    "    return smooth_part + nonsmooth_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "Next we build the neural network computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output layer\n",
    "input = tf.placeholder(tf.float32, [None, 2])\n",
    "target = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# define hidden layers with forward skip connections\n",
    "hidden = (DEPTH-1)*[None]\n",
    "hidden[0] = input\n",
    "for l in range(DEPTH-2):\n",
    "    if l>0:\n",
    "        hidden[l+1] = tf.layers.dense(\n",
    "            tf.concat(hidden[:l+1], axis=1),\n",
    "            WIDTH,\n",
    "            activation=ACTIVATION\n",
    "        )\n",
    "    else:\n",
    "        hidden[l+1] = tf.layers.dense(\n",
    "            hidden[l],\n",
    "            WIDTH,\n",
    "            activation=ACTIVATION\n",
    "        )\n",
    "\n",
    "# final layer without ReLU activation\n",
    "prediction = tf.layers.dense(tf.concat(hidden, axis=1), 1, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training procedure\n",
    "\n",
    "Next we define the loss, optimizer, descent steps, and other variable we want to keep track of during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use decaying learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    INIT_L_RATE,\n",
    "    global_step,\n",
    "    1,\n",
    "    np.exp(np.log(FINAL_L_RATE/INIT_L_RATE) / NUM_ITER),\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# L2 loss function\n",
    "# (we want L_inf error, but use smooth L2 error for optimization instead)\n",
    "loss = 1/2*tf.reduce_mean(tf.square(prediction-target))\n",
    "\n",
    "# Linf error (we can however still monitor this)\n",
    "error = tf.reduce_max(tf.abs(prediction-target))\n",
    "\n",
    "# use gradient descent during optimization\n",
    "step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "    loss,\n",
    "    global_step=global_step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookkeeping\n",
    "\n",
    "Let us take a short break and print some information to get an overview of the constructed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n----------------------------------------------------') \n",
    "print(' RUNNING EXPERIMENT WITH THE FOLLOWING PARAMETERS: ')\n",
    "print('----------------------------------------------------\\n')\n",
    "print('depth:\\t\\t\\t{}'.format(DEPTH))\n",
    "print('width:\\t\\t\\t{}'.format(WIDTH))\n",
    "print('number of neurons:\\t{}'.format(2+(DEPTH-2)*WIDTH+1))\n",
    "print('number of connections:\\t{}'.format(2+(DEPTH-2)*WIDTH*3+WIDTH*WIDTH*(DEPTH-3)*(DEPTH-2)//2))\n",
    "print('activation:\\t\\t{}'.format(ACTIVATION.__name__))\n",
    "print('learning rate:\\t\\t{} to {}'.format(INIT_L_RATE, FINAL_L_RATE))\n",
    "print('iterations:\\t\\t{}'.format(NUM_ITER))\n",
    "print('batch size:\\t\\t{}'.format(BATCH_SIZE))\n",
    "print('regularity:\\t\\t{}'.format(REGULARITY))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training\n",
    "\n",
    "Let us now start the session and run the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Tensorflow session and initialize all network variables\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# run gradient descent steps\n",
    "print('\\nStarted training...')\n",
    "print('{:8s}\\t{:8s}\\t{:8s}'.format('iter', 'l2-loss', 'linf-err'))\n",
    "print('{:8s}\\t{:8s}\\t{:8s}'.format(*(3*[8*'-'])))\n",
    "for iter in range(NUM_ITER):\n",
    "    # generate random batch of inputs and corresponding target values\n",
    "    input_batch = [DOMAIN[1]-DOMAIN[0], DOMAIN[3]-DOMAIN[2]] \\\n",
    "                  * np.random.rand(BATCH_SIZE, 2) \\\n",
    "                  + [DOMAIN[0], DOMAIN[2]]\n",
    "    target_batch = np.reshape(\n",
    "        target_func(input_batch[:, 0], input_batch[:, 1]),\n",
    "        [-1, 1]\n",
    "    )\n",
    "\n",
    "    # take gradient descent step and compute loss & error\n",
    "    loss_val, error_val, _ = session.run(\n",
    "        [loss, error, step],\n",
    "        feed_dict={input: input_batch, target: target_batch}\n",
    "    )\n",
    "    if iter % 100 == 0:\n",
    "        print('{:8d}\\t{:1.2e}\\t{:1.2e}'.format(iter, loss_val, error_val))\n",
    "print('...finished training.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trainined network\n",
    "\n",
    "After finishing the training phase it is time to check what our network has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate full sample grid of input domain\n",
    "xrange = np.linspace(DOMAIN[0], DOMAIN[1], num=RESOLUTION)\n",
    "yrange = np.linspace(DOMAIN[2], DOMAIN[3], num=RESOLUTION)\n",
    "xgrid, ygrid = np.meshgrid(xrange, yrange)\n",
    "input_test_batch = np.stack([xgrid.flatten(), ygrid.flatten()], axis=1)\n",
    "\n",
    "# get model predictions\n",
    "prediction_test_batch = np.reshape(\n",
    "    session.run(\n",
    "        prediction,\n",
    "        feed_dict={input: input_test_batch}\n",
    "    ),\n",
    "    xgrid.shape\n",
    ")\n",
    "\n",
    "# get actual target values and compare with predictions\n",
    "target_test_batch = target_func(xgrid, ygrid)\n",
    "l2_err = 1/2*np.mean(np.square(prediction_test_batch-target_test_batch))\n",
    "linf_err = np.max(np.abs(prediction_test_batch-target_test_batch))\n",
    "print(\n",
    "    'Error of predictions after training, evaluated on {}x{} grid:'\n",
    "    .format(RESOLUTION, RESOLUTION)\n",
    ")\n",
    "print('l2:\\t{:1.4e}'.format(l2_err))\n",
    "print('l2inf:\\t{:1.4e}'.format(linf_err))\n",
    "\n",
    "# plot actual target, prediction, and comparison\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(target_test_batch, extent=DOMAIN, origin='lower')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('target function')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(prediction_test_batch, extent=DOMAIN, origin='lower')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('prediction')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.abs(target_test_batch-prediction_test_batch), extent=DOMAIN, origin='lower')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('difference')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Once we are done we can close the session, which will free memory of all our tensors. This is optional, as the session is automatically closed once we close Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
