{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating functions with deep ReLU networks\n",
    "*Practical session during the [DEADALUS Introductory Intensive Course](https://daedalus-berlin.github.io/events.html), October, 2018.*\n",
    "The content is mostly based on [D. Yarotsky, 2017](https://www.sciencedirect.com/science/article/pii/S0893608017301545).\n",
    "\n",
    "## Part I: Basic building blocks\n",
    "\n",
    "In this part we will explore how several rather **simple functions** can be approximated using ReLU networks. These can then later be used to build more complex function approximations. \n",
    "\n",
    "The simple functions include:\n",
    "* a **hat** function\n",
    "* **sawtooth** functions\n",
    "* the **square** function\n",
    "* **multiplication**\n",
    "* **partition of unity** by piecewise linear bump functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from ipywidgets import interactive_output, HBox, VBox, FloatSlider, IntSlider\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLUs & hat functions\n",
    "\n",
    "Let us start by recalling that the **ReLU** activation function is simply a continuous piecewise linear function with one breakpoint at the origin, defined as\n",
    "\n",
    "$$ \\mathrm{relu}(x) = \\max\\{0, x\\}. $$\n",
    "\n",
    "The ReLU function can be used to build other continuous piecewise linear functions with more than one breakpoint, by taking **linear combinations** of several shifted ReLUs. These operations can be performed by shallow neural networks.\n",
    "\n",
    "Below, we show a linear combination of three shifted ReLU functions\n",
    "\n",
    "$$ w_1 \\mathrm{relu}(x-b_1) + w_2 \\mathrm{relu}(x-b_2) + w_3 \\mathrm{relu}(x-b_3).$$\n",
    "\n",
    "Play around with the sliders below, to see how the **weights** $w$ and **shifts (biases)** $b$ affect the result. Note, that setting \n",
    "\n",
    "$$w_1=w_3=2,\\\\ w_2=-4,\\\\ b_1=0,\\\\ b_2=\\frac{1}{2},\\\\ b_3=1,$$\n",
    "\n",
    "yields a hat function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy vectorized ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup domain and plotting grid points\n",
    "domain = [-2.0, 2.0]\n",
    "resolution = 100\n",
    "xs = np.linspace(domain[0], domain[1], resolution)\n",
    "\n",
    "# create sliders for interactive plot\n",
    "w1_slider = FloatSlider(2.0, min=-4.0, max=4.0, description='$w_1$', style={'handle_color':'red'})\n",
    "w2_slider = FloatSlider(-4.0, min=-4.0, max=4.0, description='$w_2$', style={'handle_color':'green'})\n",
    "w3_slider = FloatSlider(2.0, min=-4.0, max=4.0, description='$w_3$', style={'handle_color':'blue'})\n",
    "b1_slider = FloatSlider(0.0, min=-2.0, max=2.0, description='$b_1$', style={'handle_color':'red'})\n",
    "b2_slider = FloatSlider(0.5, min=-2.0, max=2.0, description='$b_2$', style={'handle_color':'green'})\n",
    "b3_slider = FloatSlider(1.0, min=-2.0, max=2.0, description='$b_3$', style={'handle_color':'blue'})\n",
    "\n",
    "# prepare plots of different ReLU functions\n",
    "def plot_relus(w1, b1, w2, b2, w3, b3):\n",
    "    ys = relu(xs)\n",
    "    ys1 = relu(xs-b1)\n",
    "    ys2 = relu(xs-b2)\n",
    "    ys3 = relu(xs-b3)\n",
    "    plt.plot(\n",
    "        xs, ys, 'k-',\n",
    "        xs, ys1, 'r--',\n",
    "        xs, ys2, 'g--',\n",
    "        xs, ys3, 'b--',\n",
    "        xs, w1*ys1+w2*ys2+w3*ys3, 'm-',\n",
    "        linewidth=4,\n",
    "    )\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.legend(('$\\mathrm{relu}(x)$',\n",
    "                '$\\mathrm{relu}(x-b_1)$',\n",
    "                '$\\mathrm{relu}(x-b_2)$',\n",
    "                '$\\mathrm{relu}(x-b_3)$',\n",
    "                '$\\sum_i w_i \\mathrm{relu}(x-b_i)$'), loc='best')\n",
    "    plt.ylim(-1, 3.0)\n",
    "    plt.title('linear combination of shifted ReLUs')\n",
    "\n",
    "# show interactive plot\n",
    "plot = interactive_output(plot_relus, {\n",
    "    'w1':w1_slider, 'w2':w2_slider, 'w3':w3_slider, 'b1':b1_slider, 'b2':b2_slider, 'b3':b3_slider\n",
    "})\n",
    "HBox([plot, VBox([w1_slider, w2_slider, w3_slider, b1_slider, b2_slider, b3_slider])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sawtooth functions\n",
    "\n",
    "Let us denote this **hat function** by $g$, that is\n",
    "\n",
    "$$ g(x) = 2\\,\\mathrm{relu}\\left(x\\right) - 4\\,\\mathrm{relu}\\left(x-\\frac{1}{2}\\right) + 2\\,\\mathrm{relu}\\left(x-1\\right). $$\n",
    "\n",
    "We can then, by repeatedly composing $g$ with itself, generate so called **sawtooth functions**\n",
    "\n",
    "$$ g_s(x) = \\underbrace{\\left(g\\circ g\\circ \\dots \\circ g\\right)}_{s\\text{ times }}(x),\\quad s=1, 2, \\dots .$$\n",
    "\n",
    "The function $g_s$ has $2^{s-1}$ equidistant \"teeth\" in the interval $[0, 1]$. In order to efficiently represent all the composition functions $g_s$ simultaneously by ReLU networks it is convenient to allow so called **skip connections** in the network architectures. This means, that every layer in the network can have connection to **all** following layers, not only the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy vectorized hat and sawtooth functions\n",
    "def g(x):\n",
    "    return 2*relu(x) - 4*relu(x-0.5) + 2*relu(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup domain and plotting grid points\n",
    "domain = [0.0, 1.0]\n",
    "resolution = 400\n",
    "xs = np.linspace(domain[0], domain[1], resolution)\n",
    "\n",
    "# plot sawtooth functions\n",
    "ys1 = g(xs)\n",
    "ys2 = g(ys1)\n",
    "ys3 = g(ys2)\n",
    "ys4 = g(ys3)\n",
    "plt.plot(\n",
    "    xs, ys1, 'k-',\n",
    "    xs, ys2, 'r-',\n",
    "    xs, ys3, 'g-',\n",
    "    xs, ys4, 'b-',\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('sawtooth functions')\n",
    "plt.legend(('$g_1$', '$g_2$', '$g_3$', '$g_4$'), loc='upper left', bbox_to_anchor=(1.0, 1.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The square function\n",
    "\n",
    "We will now find out for what these sawtooth functions $g_s$ can be useful. For this, consider the **square function**\n",
    "\n",
    "$$ f(x) = x^2. $$\n",
    "\n",
    "The square function will come in handy for approximating the multiplication function $\\times\\colon [0,1]^2 \\rightarrow [0,1]\\colon (x,y) \\mapsto xy$ later.\n",
    "\n",
    "We have already discussed, that piecewise linear functions can be build from ReLU networks, so it seems natural to try to approximate the square function $f$ by piecewise linear functions. Denote by $f_m$ for $m=1, 2, \\dots$ the **piecewise linear interpolation** of $f$ with $2^m + 1$ equidistant breakpoints in the interval $[0,1]$. That is \n",
    "\n",
    "$$ f_m\\left(\\frac{k}{2^m}\\right) = \\left(\\frac{k}{2^m}\\right)^2,\\quad k=0,\\dots, 2^m,$$\n",
    "\n",
    "and $f_m$ is affine linear in between the breakpoints. Note that the difference between two consecutive interpolations $f_m$ and $f_{m-1}$ is a scaled version of the sawtooth function $g_m$. In fact,\n",
    "\n",
    "$$f_m(x) = x - \\sum_{s=1}^{m} \\frac{g_s(x)}{2^{2s}}.$$\n",
    "\n",
    "As each of the $g_s$ can be represented by a ReLU network and we only take linear combinations of $g_s$ and the identity function, we see that $f_m$ can be represented by a ReLU network. Finally, as $f_m$ converges uniformly to $f$ on $[0,1]$, we can approximate $f$ arbitrarily well by a ReLU network. \n",
    "\n",
    "The **size** of such a network (number of neurons, number of connections, number of weights) which approximates $f$ up to $\\epsilon>0$ is bounded by \n",
    "\n",
    "$$ \\mathcal{O}\\left(\\ln\\frac{1}{\\epsilon}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup domain and plotting grid points\n",
    "domain = [0.0, 1.0]\n",
    "resolution = 400\n",
    "xs = np.linspace(domain[0], domain[1], resolution)\n",
    "xs0 = np.linspace(domain[0], domain[1], 2)\n",
    "xs1 = np.linspace(domain[0], domain[1], 3)\n",
    "xs2 = np.linspace(domain[0], domain[1], 5)\n",
    "\n",
    "# plot square functions and approximations\n",
    "ys = np.square(xs)\n",
    "ys0 = np.square(xs0)\n",
    "ys1 = np.square(xs1)\n",
    "ys2 = np.square(xs2)\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(\n",
    "    xs, ys, 'k-',\n",
    "    xs0, ys0, 'r--',\n",
    "    xs1, ys1, 'g--',\n",
    "    xs2, ys2, 'b--',\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend(('$x^2$', '$f_0$', '$f_1$', '$f_2$'), loc='upper left', bbox_to_anchor=(1.0, 1.0));\n",
    "plt.title('piecewise linear interpolations')\n",
    "\n",
    "# plot approximation differences\n",
    "f0 = interp1d(xs0,ys0)\n",
    "f1 = interp1d(xs1,ys1)\n",
    "f2 = interp1d(xs2,ys2)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(\n",
    "    xs, f1(xs) - f0(xs), 'm-',\n",
    "    xs, f2(xs) - f1(xs), 'c--',\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend(('$f_1-f_0$', '$f_2-f_1$'), loc='upper left', bbox_to_anchor=(1.0, 1.0));\n",
    "plt.title('differences between consecutive interpolations')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup domain and plotting grid points\n",
    "domain = [0.0, 1.0]\n",
    "resolution = 200\n",
    "xs = np.linspace(domain[0], domain[1], resolution)\n",
    "\n",
    "# numpy vectorized square approximation function\n",
    "def asquare(x, m):\n",
    "    ysa = np.copy(x)\n",
    "    gsi = np.copy(x)\n",
    "    for i in range(1,m+1):\n",
    "        gsi = g(gsi)\n",
    "        ysa -= gsi/(2**(2*i))\n",
    "    return ysa\n",
    "\n",
    "# create slider for interactive plot\n",
    "m_slider = IntSlider(1, min=0, max=5, description='$m$', style={'handle_color':'green'})\n",
    "\n",
    "# prepare interactive plot\n",
    "def plot_asquare(m=m_slider):\n",
    "    ys = np.square(xs)\n",
    "    ysa = asquare(xs, m)\n",
    "    plt.plot(\n",
    "        xs, ys, 'k-',\n",
    "        xs, ysa, 'g-',\n",
    "        xs, ysa - ys, 'r--',\n",
    "        linewidth=3\n",
    "    )\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.legend(('$x^2$', '$f_m(x), m={}$'.format(m), '$f_m(x)-x^2$'), loc='best')\n",
    "    plt.title('approximating the square function')\n",
    "\n",
    "# show interactive plot\n",
    "plot = interactive_output(plot_asquare, {'m': m_slider})    \n",
    "HBox([plot, m_slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication\n",
    "\n",
    "Finally, let us consider the **multiplication function** already mentioned before. Using the identity\n",
    "\n",
    "$$ xy = \\frac{1}{2}\\left((x+y)^2 -x^2 -y^2\\right) $$\n",
    "\n",
    "we can now approximate multiplication using linear combinations of $f$ evaluated at linear combinations of $x$ and $y$. We already know how to approximate the square function by ReLU networks, thus altogether we can approximate also multiplication by ReLU networks. \n",
    "\n",
    "*Remark 1: This only increases the networks size by a constant compared to the approximation of $f$ by a network.*\n",
    "\n",
    "*Remark 2: So far we have restricted our attention to the domain $[0,1]$ or $[0,1]^2$. However, it is not hard to see that by appropriate rescaling this can be extended to arbitrary bounded domains $[-M, M]^2$, in which case the network size increases to* \n",
    "\n",
    "$$ \\mathcal{O}\\left(\\ln\\frac{1}{\\epsilon} + \\ln M\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup domain and plotting grid points\n",
    "domain = [0.0, 1.0, 0.0, 1.0]\n",
    "resolution = 100\n",
    "xs = np.linspace(domain[0], domain[1], resolution)\n",
    "ys = np.linspace(domain[2], domain[3], resolution)\n",
    "xgrid, ygrid = np.meshgrid(xs, ys)\n",
    "\n",
    "# create slider for interactive plot\n",
    "m_slider = IntSlider(1, min=0, max=5, description='$m$')\n",
    "\n",
    "# prepare interactive plot of multiplication function\n",
    "def plot_mult(m=m_slider):\n",
    "\n",
    "    # plot multiplication function\n",
    "    zs = xgrid*ygrid\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(\n",
    "        zs,\n",
    "        extent=domain,\n",
    "        origin='lower',\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "    )\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title('multiplication $xy$')\n",
    "\n",
    "    # plot multiplication approximation \n",
    "    zsa = (asquare(np.abs(xgrid+ygrid)/2, m) - asquare(np.abs(xgrid)/2, m) - asquare(np.abs(ygrid)/2, m))*2 \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(\n",
    "        zsa, \n",
    "        extent=domain,\n",
    "        origin='lower',\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "    )\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title('approximation')\n",
    "\n",
    "    # plot difference\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(\n",
    "        np.abs(zsa-zs), \n",
    "        extent=domain,\n",
    "        origin='lower',\n",
    "        cmap=plt.get_cmap('Greys_r'),\n",
    "        vmin=0.0,\n",
    "        vmax=0.2,\n",
    "    )\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title('error')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    \n",
    "# show interactive plot\n",
    "plot = interactive_output(plot_mult, {'m': m_slider})\n",
    "HBox([plot, m_slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition of unity\n",
    "We now have the basic building blocks ready to use for approximating more complicated functions. The main idea is to use the multiplication operation to build **multivariate polynomials**, and then in turn use these polynomials to build **local Taylor expansions** of the target functions that we want to approximate. \n",
    "\n",
    "In order to make this work we need a method to localize polynomials. It is not hard to see how piecewise-linear **bump functions** can be constructed from ReLUs in a way similar to the construction of the hat function seen before. \n",
    "\n",
    "$$ \\mathrm{bump}(x) = \\mathrm{relu}(x+2) - \\mathrm{relu}(x+1) - \\mathrm{relu}(x-1) + \\mathrm{relu}(x-2) $$\n",
    "\n",
    "These bump functions together with the multiplication operation can be used to build multivariate localized bumps that form a **partition of unity** on $[0,1]$.\n",
    "\n",
    "$$ \\mathrm{bump}_{k,N}\\left(3N\\left(x-\\frac{k}{N}\\right)\\right),\\quad k=0,\\dots,N,\\quad N\\in\\mathbb{N} $$\n",
    "\n",
    "This is illustrated for the one-dimensional case below.\n",
    "\n",
    "A general function expansion can then be written in the form\n",
    "\n",
    "$$ \\sum_k \\sum_n a_{k,n} \\mathrm{bump}_{k,N}(x)\\left(x-\\frac{k}{N}\\right)^n $$\n",
    "\n",
    "for some coefficients $a_{k,n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup domain and plotting grid points\n",
    "domain = [0.0, 1.0]\n",
    "resolution = 400\n",
    "xs = np.linspace(domain[0], domain[1], resolution)\n",
    "\n",
    "# create slider for interactive plot\n",
    "N_slider = IntSlider(3, min=1, max=8, description='$N$')\n",
    "\n",
    "# prepare interactive plot of multiplication function\n",
    "bump = lambda x: relu(x+2) - relu(x+1) - relu(x-1) + relu(x-2)\n",
    "def plot_bumps(N=N_slider):\n",
    "    for k in range(N+1):\n",
    "        plt.plot(xs, bump(3*N*(xs-k/N)))\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title('partition of unity')\n",
    "\n",
    "# show interactive plot\n",
    "plot = interactive_output(plot_bumps, {'N': N_slider})\n",
    "HBox([plot, N_slider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
